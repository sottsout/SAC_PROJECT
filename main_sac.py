# import pybullet_envs
# from flat_wrapper import ObsOnlyWrapper
#
# import numpy as np
# from sac_torch import Agent
# from utils import plot_learning_curve
# from gym import wrappers
# import os
# import warnings
# import torch
# warnings.filterwarnings("ignore", category=DeprecationWarning)  # Optional
# import gymnasium as gym
#
# import panda_gym
#
# if __name__ == '__main__':
#
#     print("Running main...")
#     ###############################DEBUGGING#########################
#     print("CUDA available:", torch.cuda.is_available())
#     print("PyTorch CUDA version:", torch.version.cuda)
#     print("Device name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")
#     ###############################DEBUGGING#########################
#
#     os.makedirs("tmp", exist_ok=True)
#     os.makedirs("plots", exist_ok=True)
#     os.makedirs("tmp/video", exist_ok=True)
#
#     warnings.filterwarnings("ignore", category=UserWarning)
#     warnings.filterwarnings("ignore", category=DeprecationWarning)
#
#
#     #env = gym.make('PandaReachDense-v3')
#     #env = gym.make("PandaPushDense-v3", render_mode="human")
#     # env = gym.make(
#     #     "PandaPushDense-v3",
#     #     control_type="joints",  # Î® "torque"
#     #     reward_type="dense",
#     #     render_mode="human"
#     # )
#     base_env = gym.make(
#         "PandaPickAndPlaceJoints-v3",
#         control_type="joints",  # Î® "torque"
#         reward_type="dense",
#         render_mode="human"
#     )
#     env = ObsOnlyWrapper(base_env)
#     #agent = Agent(input_dims=env.observation_space.shape, env=env,
#     #        n_actions=env.action_space.shape[0])
#
#     #obs_shape = env.observation_space["observation"].shape  # tuple Ï€.Ï‡. (30,)
#     obs_shape = env.observation_space.shape
#     n_actions = env.action_space.shape[0]
#
#     #agent = Agent(input_dims=obs_shape, env=env, n_actions=n_actions)
#     agent = Agent(
#         input_dims=env.observation_space.shape,  # Ï€.Ï‡. (30,)
#         env=env,
#         n_actions=env.action_space.shape[0]
#     )
#     n_games = 100000
#     # uncomment this line and do a mkdir tmp && mkdir video if you want to
#     # record video of the agent playing the game.
#     #env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)
#     filename = 'inverted_pendulum.png'
#
#     figure_file = 'plots/' + filename
#
#     best_score = -np.inf          # Î±Î½Ï„Î¯ Î³Î¹Î± env.reward_range[0]
#     score_history = []
#     load_checkpoint = False
#
#     if load_checkpoint:
#         agent.load_models()
#         env.render(mode='human')
#
#     for i in range(n_games):
#         obs, info = env.reset()  # obs ÎµÎ¯Î½Î±Î¹ Ï„Î¿ flat vector
#         done = False
#         score = 0.0
#
#         while not done:
#             action = agent.choose_action(obs)
#
#             obs_, reward, terminated, truncated, info = env.step(action)
#             done = terminated or truncated
#
#             agent.remember(obs, action, reward, obs_, done)
#             if not load_checkpoint:
#                 agent.learn()
#
#             score += reward
#             obs = obs_  # move to next state
#
#         score_history.append(score)
#         avg_score = np.mean(score_history[-100:])
#
#         if avg_score > best_score:
#             best_score = avg_score
#             if not load_checkpoint:
#                 agent.save_models()
#
#         print(f"episode {i}  score {score:.1f}  avg_score {avg_score:.1f}")
#
#     if not load_checkpoint:
#         x = [i+1 for i in range(n_games)]
#         plot_learning_curve(x, score_history, figure_file)


"""
SAC training script for PandaPickAndPlaceJointsâ€‘v3
--------------------------------------------------
* Flat observations via `ObsOnlyWrapper`
* Dense reward, joints control
* Video recorded **ÎºÎ¬Î¸Îµ 1â€¯000 ÎµÏ€ÎµÎ¹ÏƒÏŒÎ´Î¹Î±**
* Models Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ ÏŒÏ„Î±Î½ Î¿ ÂµÎ­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚ Ï„Ï‰Î½ 100 Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Ï‰Î½ ÎµÏ€ÎµÎ¹ÏƒÎ¿Î´Î¯Ï‰Î½ Î²ÎµÎ»Ï„Î¹ÏÎ½ÎµÏ„Î±Î¹
"""

# import os
# import warnings
# from pathlib import Path
#
# import gymnasium as gym
# from gymnasium.wrappers import RecordVideo
# import numpy as np
# import torch
#
# import panda_gym  # âŸµ registration sideâ€‘effect
#
# from flat_wrapper import ObsOnlyWrapper  # Î´Î¹ÎºÏŒÏ‚ ÏƒÎ¿Ï… wrapper (flat obs)
# from sac_torch import Agent
# from utils import plot_learning_curve
#
#
# # ----------------------------------------------------------------------------
# # 1.  Video trigger (ÎºÎ¬Î¸Îµ 1000 ÎµÏ€ÎµÎ¹ÏƒÏŒÎ´Î¹Î±)
# # ----------------------------------------------------------------------------
# VIDEO_INTERVAL = 1000
# video_folder = Path("tmp/video")
# video_folder.mkdir(parents=True, exist_ok=True)
#
# def video_trigger(episode_id: int) -> bool:
#     return episode_id % VIDEO_INTERVAL == 0
#
#
# # ----------------------------------------------------------------------------
# # 2.  Create environment (wrapped + video)
# # ----------------------------------------------------------------------------
# base_env = gym.make(
#     "PandaPickAndPlaceJoints-v3",
#     control_type="joints",
#     reward_type="dense",
#     render_mode="rgb_array",   # Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿ Î³Î¹Î± RecordVideo
# )
#
# env = RecordVideo(
#     ObsOnlyWrapper(base_env),
#     video_folder=str(video_folder),
#     episode_trigger=video_trigger,
# )
#
#
# # ----------------------------------------------------------------------------
# # 3.  Agent initialization
# # ----------------------------------------------------------------------------
# obs_shape = env.observation_space.shape  # e.g. (30,)
# agent = Agent(
#     input_dims=obs_shape,
#     env=env,
#     n_actions=env.action_space.shape[0],
# )
#
#
# # ----------------------------------------------------------------------------
# # 4.  Training loop
# # ----------------------------------------------------------------------------
# N_GAMES = 60_000
# BEST_SCORE = -np.inf
# scores, avg_scores = [], []
#
# print("CUDA available:", torch.cuda.is_available())
# print("Device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
#
# for episode in range(N_GAMES):
#     obs, _ = env.reset()
#     done, score = False, 0.0
#
#     while not done:
#         action = agent.choose_action(obs)
#         obs_, reward, terminated, truncated, _ = env.step(action)
#         done = terminated or truncated
#
#         # store + learn
#         agent.remember(obs, action, reward, obs_, done)
#         agent.learn()
#
#         obs = obs_
#         score += reward
#
#     scores.append(score)
#     avg_score = np.mean(scores[-100:])
#     avg_scores.append(avg_score)
#
#     if avg_score > BEST_SCORE:
#         BEST_SCORE = avg_score
#         agent.save_models()
#
#     if episode % 10 == 0:
#         print(f"Episode {episode:>6} | Score: {score:>7.2f} | Avg100: {avg_score:>7.2f}")
#
# # ----------------------------------------------------------------------------
# # 5.  Plot learning curve
# # ----------------------------------------------------------------------------
# x = [i + 1 for i in range(len(scores))]
# plot_learning_curve(x, scores, "plots/sac_pickplace.png")
#
# print("Training finished. Videos saved to", video_folder)


# Two-Stage SAC Training: Reach âœ PickAndPlace
# --------------------------------------------
# * 1Î· Ï†Î¬ÏƒÎ·: PandaReachDense-v3
# * 2Î· Ï†Î¬ÏƒÎ·: PandaPickAndPlaceJoints-v3
# * Î§ÏÎ®ÏƒÎ· flat Ï€Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÏ‰Î½ (ObsOnlyWrapper)
# * ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Î²Î¯Î½Ï„ÎµÎ¿ Î¼ÏŒÎ½Î¿ ÏƒÏ„Î· 2Î· Ï†Î¬ÏƒÎ·

# ---------------------------------------------
# main_twostage_partial.py
# ---------------------------------------------
"""
Two-Stage SAC Training with *Partial* Weight Transfer
Phase-1  : PandaReachDense-v3
Phase-2  : PandaPickAndPlaceJoints-v3
Transfer : Î¦Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ ÎœÎŸÎÎŸ Ï„Î± Î²Î¬ÏÎ· Ï€Î¿Ï… Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ ÏƒÎµ ÏƒÏ‡Î®Î¼Î±
           (Ï„Î± fc1 layers ÎµÏ€Î±Î½ÎµÎºÎºÎ¹Î½Î¿ÏÎ½Ï„Î±Î¹).
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"   # quick fix Î³Î¹Î± OpenMP

import warnings
from pathlib import Path
from typing import Dict
from stable_baselines3.common.logger import configure

import numpy as np
import torch
import gymnasium as gym
from gymnasium.wrappers import RecordVideo

import panda_gym                         # register Panda envs
from flat_wrapper import ObsOnlyWrapper  # flattens obs dictionaries
from sac_torch import Agent
from utils import plot_learning_curve

# Example: save logs ÏƒÏ„Î¿ Ï†Î¬ÎºÎµÎ»Î¿ "logs/"
log_dir = "logs/"
new_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])

# ------------------------------------------------------------------
# 0.  paths & hyper-params
# ------------------------------------------------------------------
REACH_GAMES      = 10_000
PICKPLACE_GAMES  = 50_000
CHECKPOINT_DIR   = Path("tmp/sac")      # ÏŒÏ€Î¿Ï… Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Î¿ Agent
VIDEO_DIR        = Path("tmp/video")
PLOTS_DIR        = Path("plots")

for p in (CHECKPOINT_DIR, VIDEO_DIR, PLOTS_DIR):
    p.mkdir(parents=True, exist_ok=True)

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

print("Running 2-stage SAC training (partial transfer)...")
print("CUDA:", torch.cuda.is_available(),
      "| Device:",
      torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")


# ------------------------------------------------------------------
# helper: generic training loop
# ------------------------------------------------------------------
def train_agent(env, agent, n_games: int,
                plot_name: str,
                save_every: int = 100) -> None:
    """Train *agent* on *env* for *n_games* episodes."""

    scores, successes = [], []
    best = -np.inf
    early_stop = False

    for ep in range(n_games):
        obs, _ = env.reset()
        done, ep_return = False, 0.0
        ep_success = 0

        while not done:
            action = agent.choose_action(obs)
            obs_, reward, terminated, truncated, info = env.step(action)

            done = terminated or truncated

            agent.remember(obs, action, reward, obs_, done)
            agent.learn()

            obs = obs_
            ep_return += reward
            ep_success = info.get("is_success", ep_success)

        # Metrics
        scores.append(ep_return)
        successes.append(ep_success)
        avg_score = np.mean(scores[-100:])
        avg_success = np.mean(successes[-100:])

        # Logging
        if agent.logger is not None:
            agent.logger.record("rollout/ep_rew_mean", avg_score)
            agent.logger.record("rollout/success_rate", avg_success)
            agent.logger.record("time/episodes", ep)
            agent.logger.dump(step=ep)

        # Save models if best
        if avg_score > best:
            best = avg_score
            agent.save_models()

        # Entropy freeze if high success
        if agent.automatic_entropy_tuning and avg_success > 0.50:
            agent.automatic_entropy_tuning = False
            agent.alpha = agent.log_alpha.exp().item()
            print(f"ğŸ”’  Fixing Î± = {agent.alpha:.5f} (success={avg_success:.2f})")

        # Early stopping
        if avg_success > 0.95 and ep >= 200:
            print(f"ğŸ¯ Early stopping: success rate {avg_success:.2f} at episode {ep}")
            early_stop = True
            agent.save_models()
            break

        # Extra model saving
        if save_every and ep % save_every == 0:
            agent.save_models()

        # Progress print
        if ep % 10 == 0:
            print(f"Ep {ep:5d} | Return: {ep_return:7.2f} | "
                  f"Avg100: {avg_score:7.2f} | "
                  f"Succ100: {avg_success:.2f}")

    # Plot learning curve
    xs = list(range(1, len(scores) + 1))
    plot_learning_curve(xs, scores, str(PLOTS_DIR / plot_name))

    if early_stop:
        print("âœ… Early Stopping Triggered.")

    # learning curve
    xs = list(range(1, len(scores) + 1))
    plot_learning_curve(xs, scores, str(PLOTS_DIR / plot_name))


# ------------------------------------------------------------------
# helper: partial-load (filter state-dict by matching shapes)
# ------------------------------------------------------------------
def _filter_state(target_net: torch.nn.Module,
                  preload: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
    """Return only those params from *preload* Ï€Î¿Ï… Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ ÏƒÎµ *target_net*."""
    keep = {}
    tgt_sd = target_net.state_dict()
    for k, v in preload.items():
        if k in tgt_sd and tgt_sd[k].shape == v.shape:
            keep[k] = v
    return keep


def load_partial_weights(pick_agent: Agent, ckpt_dir: Path = CHECKPOINT_DIR) -> None:
    files = {
        "actor":    ckpt_dir / "actor",
        "critic_1": ckpt_dir / "critic_1",
        "critic_2": ckpt_dir / "critic_2",
    }

    # actor
    sd = torch.load(files["actor"])
    pick_agent.actor.load_state_dict(_filter_state(pick_agent.actor, sd), strict=False)

    # critics
    for net_name in ("critic_1", "critic_2"):
        sd = torch.load(files[net_name])
        getattr(pick_agent, net_name).load_state_dict(
            _filter_state(getattr(pick_agent, net_name), sd), strict=False
        )

    print("âœ“ Partial weights loaded (non-matching layers re-initialized).")


# ------------------------------------------------------------------
# 1. Phase-1: Reach
# ------------------------------------------------------------------
print("\nâ€” Phase 1: PandaReach-v3 â€”")
reach_env = ObsOnlyWrapper(gym.make(
    "PandaReach-v3",
    reward_type="sparse",
    render_mode="human"        # Î® "rgb_array"
))
reach_agent = Agent(
    input_dims=reach_env.observation_space.shape,
    env=reach_env,
    n_actions=reach_env.action_space.shape[0],
    automatic_entropy_tuning=True,
    logger=new_logger
)
train_agent(reach_env, reach_agent, REACH_GAMES, "sac_reach.png")


# ------------------------------------------------------------------
# 2. Phase-2: Pick&Place (Î½Î­Î¿Ï‚ agent + partial load)
# ------------------------------------------------------------------
print("\nâ€” Phase 2: PandaPickAndPlaceJoints-v3 â€”")

def trigger(ep): return ep % 1000 == 0
# pick_env = RecordVideo(
#     ObsOnlyWrapper(gym.make(
#         "PandaPickAndPlaceJoints-v3",
#         control_type="joints",
#         reward_type="dense",
#         render_mode="rgb_array"
#     )),
#     video_folder=str(VIDEO_DIR),
#     episode_trigger=trigger
# )
pick_env = ObsOnlyWrapper(gym.make(
    "PandaPickAndPlaceJoints-v3",
    control_type="joints",
    reward_type="dense",
    render_mode="human"  # Ï€Î¹Î¿ Î³ÏÎ®Î³Î¿ÏÎ¿ Î±Ï€ÏŒ "rgb_array"
))

pick_agent = Agent(
    input_dims=pick_env.observation_space.shape,   # (Ï€.Ï‡.) 19-dim
    env=pick_env,
    n_actions=pick_env.action_space.shape[0],
    automatic_entropy_tuning=True,
    logger=new_logger
)

# Ï†Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ ÏŒ,Ï„Î¹ Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î±Ï€ÏŒ Reach
load_partial_weights(pick_agent)

# â€¦ ÎºÎ±Î¹ ÏƒÏ…Î½ÎµÏ‡Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ training
train_agent(pick_env, pick_agent, PICKPLACE_GAMES, "sac_pickplace.png")


# ------------------------------------------------------------------
# 3. Done
# ------------------------------------------------------------------
print("\nâœ… Training complete.")
print("ğŸ“ˆ Reach plot      :", PLOTS_DIR / "sac_reach.png")
print("ğŸ“ˆ Pick&Place plot :", PLOTS_DIR / "sac_pickplace.png")
print("ğŸ¥ Videos @        :", VIDEO_DIR.resolve())

